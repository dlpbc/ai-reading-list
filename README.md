# reading-list

## Unread
#### [Meta Learning] - article  
https://www.cifar.ca/assets/machines-learn-new-ways-of-learning/  
*Short Note: *

#### [Synthetic Gradients] - blog post  
http://iamtrask.github.io/2017/03/21/synthetic-gradients/  
*Short Note: rather than actually compute gradient via backprop, each NN layer approximates their gradient by learning.*

#### [Information bottleneck] - article  
https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/  
*Short Note: link to an article written about a talk about potential explanation of why neural net works was given.*

#### [about-sgd] - blogpost  
https://medium.com/intuitionmachine/the-peculiar-behavior-of-deep-learning-loss-surfaces-330cb741ec17  
*Short Note: *

#### [Meta Learning] - research paper  
https://arxiv.org/abs/1709.07417  
*Short Note: neural optimizer. authors learnt new optimizers from cifar-10 dataset and claim that they generalise to other tasks.*

#### [CMU Fall 2017 NLP Course] - video  
https://www.youtube.com/watch?v=Sss2EA4hhBQ&list=PL8PYTP1V4I8ABXzdqtOpB_eqBlVAz_xPT  
*Short Note:*

#### [Learning Through Interation: Generalisation in Robot RL] - video  
https://www.youtube.com/watch?v=Ko8IBbYjdq8  
*Short Note:*

  

## Read
